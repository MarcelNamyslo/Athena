{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from linearlrp import LinearLRPLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import MNISTDataLoader\n",
    "from model import CNN, LRP_CNN, train_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_input_image(image):\n",
    "    image = image.view(28, 28).cpu().numpy()  # Reshape and convert to numpy for plotting\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relevance_scores(relevance_scores, input_image):\n",
    "    # Assuming relevance_scores is already in [28, 28] shape after the above steps\n",
    "    input_image = input_image.view(28, 28).cpu().numpy()\n",
    "\n",
    "    # Normalize the relevance scores\n",
    "    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())\n",
    "\n",
    "    # Plot the original image\n",
    "    plt.imshow(input_image, cmap='gray', alpha=0.6)\n",
    "    \n",
    "    # Overlay the relevance scores as a heatmap\n",
    "    plt.imshow(relevance_scores, cmap='jet', alpha=0.4)\n",
    "    plt.title(\"Relevance Scores Overlay\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2305\n",
      "Accuracy: 96.41%\n",
      "Epoch [2/5], Loss: 0.0934\n",
      "Accuracy: 97.53%\n",
      "Epoch [3/5], Loss: 0.0635\n",
      "Accuracy: 97.39%\n",
      "Epoch [4/5], Loss: 0.0509\n",
      "Accuracy: 97.70%\n",
      "Epoch [5/5], Loss: 0.0378\n",
      "Accuracy: 97.72%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load MNIST data\n",
    "train_loader, test_loader = MNISTDataLoader.load_mnist_data()\n",
    "\n",
    "# Initialize the model\n",
    "model1 = CNN()\n",
    "\n",
    "model = LRP_CNN()\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, test_loader, epochs=5, learning_rate=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[28, 28]' is invalid for input of size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m relevance_scores \u001b[39m=\u001b[39m model(input_image, explain\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, rule\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlrp0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[39m# Reshape and plot relevance scores\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m relevance_scores_for_input \u001b[39m=\u001b[39m relevance_scores\u001b[39m.\u001b[39;49mview(\u001b[39m28\u001b[39;49m, \u001b[39m28\u001b[39;49m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     24\u001b[0m \u001b[39m# Plot the relevance map overlaid on the input image\u001b[39;00m\n\u001b[1;32m     25\u001b[0m plot_relevance_scores(relevance_scores_for_input, input_image)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[28, 28]' is invalid for input of size 10"
     ]
    }
   ],
   "source": [
    "# Select a single image from the test set\n",
    "for images, labels in test_loader:\n",
    "    input_image = images[0].unsqueeze(0)  # Select the first image and add batch dimension\n",
    "    break\n",
    "\n",
    "# Assuming 'input_image' is your input tensor\n",
    "plot_input_image(input_image)\n",
    "\n",
    "# Apply LRP\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "output = model(input_image, explain=True, rule='lrp0')\n",
    "predicted_class = output.argmax(dim=1).item()\n",
    "\n",
    "# To get the relevance map, we need to backpropagate from the predicted class\n",
    "relevance_scores = torch.zeros_like(output)\n",
    "relevance_scores[0, predicted_class] = 1.0  # Start with 1 for the predicted class\n",
    "\n",
    "# Backpropagate relevance scores to the input\n",
    "relevance_scores = model(input_image, explain=True, rule='lrp0')\n",
    "\n",
    "# Reshape and plot relevance scores\n",
    "relevance_scores_for_input = relevance_scores.view(28, 28).cpu().detach().numpy()\n",
    "\n",
    "# Plot the relevance map overlaid on the input image\n",
    "plot_relevance_scores(relevance_scores_for_input, input_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonvenv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "917e879ce2cb49910bde7bbfe75d730b899cefacb86b31c9ca8272dd0c98dcad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
